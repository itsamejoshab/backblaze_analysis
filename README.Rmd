---
title: "Readme"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Data Sources
I'm buying a hard drive to backup my data at home, and I want to buy a drive that's not gonna fail.  Fortunately, [BackBlaze has shared all of their data on hard drives and drive failures.](https://www.backblaze.com/b2/hard-drive-test-data.html#downloading-the-raw-hard-drive-test-data). has 

Backblaze [did their own analysis](https://www.backblaze.com/blog/backblaze-hard-drive-stats-q1-2020/) of drive failures, but I don't like their approach for 2 reasons:    
1. Their "annualized failure rate" (`Drive Failures / (Drive Days / 365)`) assumes that failure rates are constant over time.  E.g. this assumptions means that observing 1 drive for 100 days gives you the exact same information as observing 100 drives for 1 day.    
2. They don't really explain how the derived confidence intervals or used them in their analysis, and pretty much rely on the "annualized failure rate" to make conclusions.  I want to use a confidence interval for my decision making. For a lot more detail on why a confidence interval is a good idea, read Evan Miller's blog post about a different type of problem: [How Not To Sort By Average Rating](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html).    


I really wanted to use a failure model that allows for time-varying failure rates, and then pick a drive based on a confidence interval, so here we are...

# Survival Analysis

I really wanted to pick my drive based on: `lower 95% confidence interval for median time to failure`.  In other words, I want to pick the drive that has the most evidence it will last a large number of days.

In order to analyze median time to failure, you need to observe your sample long enough for 50% of the drives to fail.  However, these drives are *so reliable* that almost none of the models in the sample have yet hit the 50% failure mark.  Therefore, I will settle for looking at `upper 95% confidence interval for failure rate after 1 year`.  In other words, I want to pick the drive I am most sure will last at least one year.

Some technical notes:    
1. I only looked at drive models where at least 100 individual drives lasted a year or longer, to remove drive models without a lot of data.  (I don't love this, and wish I knew how to make the survival curve confidence intervals reflect uncertainy from the number of individuals observed).    
2. This analysis does not assume a constant failure rate.  We often see in real life that drives fail at a high rate early on, and then failures become less likely over time.    
3. This analysis allows different drive models to have different failure "curves."  I looped over every drive model, ran the `survfit` function in R (which fits a very simple, non-parametric [Kaplan-Meier survival curve](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator)), and then took the 95% confidence interval at 1 year from the fitted survival curve.    

```{r surv, echo=FALSE}
# Setup
library(data.table)
library(survival)
days_to_year <- 365.2425

data_surv <- fread('all_data.csv')
keys <- c('model', 'serial_number')
setkeyv(data_surv, keys)
data_surv <- data_surv[,list(
  N = sum(N),
  failure=sum(failure),
  capacity_tb=round(max(capacity_bytes)/1e+12, 1)
), by=keys]

# Plot survival curves for 1 or 2 models to check results make sense
# library(survminer)
# dat <- data_surv[model %in% c('HGST HUH721212ALN604','HGST HMS5C4040BLE640', 'WDC WD40EFRX'),]
# model <- dat[,survfit(Surv(N, failure) ~ model)]
# ggsurvplot(model, data=dat, fun='cumhaz', conf.int = T)
# ggsurvplot(model, data=dat, conf.int = T, ylim=c(.95, 1.0))

# Only keep models were at leat 100 drives made it to one year
data_surv[,count_one_year := sum(N>=days_to_year), by='model']
data_surv <- data_surv[count_one_year > 99,]

# Do a non-parametric survival curve for every drive model
survival_curve_at_t <- function(time, failure, at=days_to_year){
  out <- survfit(Surv(time, failure)~1)
  out <- summary(out, times=at, conf.int=.95)
  out <- list(
    surv = out$surv,
    lower =  out$lower
  )
  return(out)
}

data_surv <- data_surv[, c(list(capacity_tb=max(capacity_tb), N=.N), survival_curve_at_t(N, failure)), by='model']
data_surv <- data_surv[order(-lower),]

# Choose best drive
best_drive <- data_surv[1, model]
```

Here's the results of our analysis.  The `r best_drive` is the most reliable drive in our sample of data:
```{r surv_results, echo=FALSE}
knitr::kable(
  data_surv[,
    list(
      model, 
      capacity_tb, 
      N,
      one_year_failure_rate=sprintf("%1.2f%%", 100*(1-surv)),
      ci_95=sprintf("%1.2f%%", 100*(1-lower)))
    ]
)
```

# Replicating my results
[all_data.csv](all_data.csv) has the cleaned up data from backblaze, at the level of individual drives, says observed, and whether or not the drive failed.   

[README.Rmd](README.Rmd) has the code to run this analysis and generate this [README.md](README.md) file you are reading right now. Use [RStudio](https://rstudio.com/products/rstudio/download/) to `knit` the `Rmd` file into a `md` file, which github will then render nicely for you. `knitr::kable produces the nice table of my results.

# Erratum
![I nerd sniped myself](https://imgs.xkcd.com/comics/nerd_sniping.png)
